{"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"WtzssAZNYj2e"},"id":"WtzssAZNYj2e"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3-qIMSnkPEYY","executionInfo":{"status":"ok","timestamp":1684764998932,"user_tz":-540,"elapsed":34243,"user":{"displayName":"a jin","userId":"10861403131617240942"}},"outputId":"d1b3fabd-916b-4222-eb0d-0ec6abc10025"},"id":"3-qIMSnkPEYY","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27229,"status":"ok","timestamp":1684765026156,"user":{"displayName":"a jin","userId":"10861403131617240942"},"user_tz":-540},"id":"8e59dc52","outputId":"d005543b-28ad-4108-ed94-59d5b14b42ee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets\n","  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.10.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.7,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n","Collecting aiohttp (from datasets)\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting responses<0.19 (from datasets)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: tokenizers, xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, datasets\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.14.1 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 tokenizers-0.13.3 transformers-4.29.2 xxhash-3.2.0 yarl-1.9.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n"]}],"source":["!pip install -U transformers datasets scipy scikit-learn \n","!pip install sentencepiece"],"id":"8e59dc52"},{"cell_type":"markdown","metadata":{"id":"8wSLfX-jaIqV"},"source":["## 문장 분류 모델 학습"],"id":"8wSLfX-jaIqV"},{"cell_type":"markdown","metadata":{"id":"92e868b7-3cf7-4977-a6e7-ebbb99ba298e"},"source":["노트북을 실행하는데 필요한 라이브러리들을 모두 임포트합니다."],"id":"92e868b7-3cf7-4977-a6e7-ebbb99ba298e"},{"cell_type":"code","execution_count":null,"metadata":{"id":"243b461f"},"outputs":[],"source":["import random\n","import logging\n","from IPython.display import display, HTML\n","\n","\n","import numpy as np\n","import pandas as pd\n","import datasets\n","from datasets import load_dataset, load_metric, ClassLabel, Sequence,Dataset, DatasetDict\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, BertTokenizerFast, AlbertForSequenceClassification"],"id":"243b461f"},{"cell_type":"markdown","metadata":{"id":"59b1e5ae-471f-461f-850f-4241a6037471"},"source":["[링크 텍스트](https://)학습에 필요한 정보를 변수로 기록합니다.\n","\n","본 노트북에서는 `klue-roberta-base` 모델을 활용하지만, https://huggingface.co/klue 페이지에서 더 다양한 사전학습 언어 모델을 확인하실 수 있습니다.\n","\n","학습 태스크로는 `ynat`를, 배치 사이즈로는 64를 지정하겠습니다.\n"],"id":"59b1e5ae-471f-461f-850f-4241a6037471"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ecaacbe"},"outputs":[],"source":["model_checkpoint = 'bongsoo/albert-small-kor-v1'\n","batch_size = 64"],"id":"6ecaacbe"},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/텍스트분석2023/traindata/dataset.csv')\n","\n","df[\"label\"] = df[\"label\"].astype(int)"],"metadata":{"id":"HFIIE6wndMI5"},"id":"HFIIE6wndMI5","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QzlmWQ2DrZ7P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684586956736,"user_tz":-540,"elapsed":23,"user":{"displayName":"a jin","userId":"10861403131617240942"}},"outputId":"d5601a07-53b9-4a32-a86a-5e97a028af04"},"outputs":[{"output_type":"stream","name":"stdout","text":["                                                message  label\n","0     the movie begins in the past where a young boy...      1\n","1     emerging from the human psyche and showing cha...      1\n","2     spurning her mother's insistence that she get ...      1\n","3     amitabh can't believe the board of directors a...      1\n","4     she , among others excentricities , talks to a...      1\n","...                                                 ...    ...\n","9994  a haunted , bountiful film that demands patien...      0\n","9995  the movie's gloomy atmosphere is fascinating ,...      0\n","9996  it aimlessly and unsuccessfully attempts to fu...      0\n","9997  an authentically vague , but ultimately purpos...      0\n","9998  visually imaginative , thematically instructiv...      0\n","\n","[9999 rows x 2 columns]\n"]}],"source":["\n","\n","print(df)\n","\n","from sklearn.model_selection import train_test_split\n","                                  \n","dataset_train, dataset_val = train_test_split(df, test_size=0.25, random_state=0)\n","\n","dataset_train = dataset_train[['message','label']]\n","dataset_val = dataset_val[['message','label']]\n","\n","tds = Dataset.from_pandas(dataset_train)\n","vds = Dataset.from_pandas(dataset_val)\n","\n","datasets = DatasetDict()\n","\n","datasets['train'] = tds\n","datasets['validation'] = vds"],"id":"QzlmWQ2DrZ7P"},{"cell_type":"markdown","metadata":{"id":"f39_pKKnarD3"},"source":["각 예시 데이터는 아래와 같이 두 개의 문장과 두 문장의 추론 관계를 라벨로 지니고 있습니다."],"id":"f39_pKKnarD3"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1684586956737,"user":{"displayName":"a jin","userId":"10861403131617240942"},"user_tz":-540},"id":"e46bf44d","outputId":"83c314bb-4616-49a8-ee79-0c7c08c25875"},"outputs":[{"output_type":"stream","name":"stdout","text":["(7499, 3)\n","(2500, 3)\n"]}],"source":["print(datasets[\"train\"].shape)\n","print(datasets[\"validation\"].shape)"],"id":"e46bf44d"},{"cell_type":"code","source":["datasets[\"train\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CGQDU8gPdbdF","executionInfo":{"status":"ok","timestamp":1684586956737,"user_tz":-540,"elapsed":20,"user":{"displayName":"a jin","userId":"10861403131617240942"}},"outputId":"96dd5d26-849a-47b4-961c-7535be3cc22b"},"id":"CGQDU8gPdbdF","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['message', 'label', '__index_level_0__'],\n","    num_rows: 7499\n","})"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"pc1BXYLYaupD"},"source":["데이터셋을 전반적으로 살펴보기 위한 시각화 함수를 다음과 같이 정의합니다."],"id":"pc1BXYLYaupD"},{"cell_type":"code","execution_count":null,"metadata":{"id":"8110a3ee"},"outputs":[],"source":["datasets = datasets.remove_columns(['__index_level_0__'])"],"id":"8110a3ee"},{"cell_type":"code","source":["datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"54v7FAScdsmy","executionInfo":{"status":"ok","timestamp":1684586956738,"user_tz":-540,"elapsed":19,"user":{"displayName":"a jin","userId":"10861403131617240942"}},"outputId":"ccb8a603-9e56-4cf6-b505-1e840af47e9c"},"id":"54v7FAScdsmy","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['message', 'label'],\n","        num_rows: 7499\n","    })\n","    validation: Dataset({\n","        features: ['message', 'label'],\n","        num_rows: 2500\n","    })\n","})"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"zakMS4Y61sLm"},"source":["훈련 과정 중 모델의 성능을 파악하기 위한 메트릭을 설정합니다.\n","\n","`datasets` 라이브러리에는 이미 구현된 메트릭을 사용할 수 있는 `load_metric` 함수가 있습니다.\n","\n","그 중 f1 score 로 metric을 설정하였습니다. "],"id":"zakMS4Y61sLm"},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":417,"status":"ok","timestamp":1684586957138,"user":{"displayName":"a jin","userId":"10861403131617240942"},"user_tz":-540},"id":"5c214cfb-efa2-4e31-9268-9d1038626095","colab":{"base_uri":"https://localhost:8080/"},"outputId":"afb0117d-4838-4973-d286-20831ef94307"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-11-f6db7250f76c>:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","  metric = load_metric(\"f1\")\n"]}],"source":["#metric.inputs_description\n","metric = load_metric(\"f1\")"],"id":"5c214cfb-efa2-4e31-9268-9d1038626095"},{"cell_type":"markdown","metadata":{"id":"I07wSysL2fRs"},"source":["`accuracy` 메트릭이 정상적으로 작동하는지 확인하기 위해, 랜덤한 예측 값과 라벨 값을 생성합니다."],"id":"I07wSysL2fRs"},{"cell_type":"markdown","metadata":{"id":"rPvvy_wC24i_"},"source":["이제 학습에 활용할 토크나이저를 로드해오도록 합니다."],"id":"rPvvy_wC24i_"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b64e70af"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=False)"],"id":"b64e70af"},{"cell_type":"markdown","metadata":{"id":"YIVjwvm428Q_"},"source":["로드된 토크나이저가 두 개 문장을 토큰화하는 방식을 파악하기 위해 두 문장을 입력 값으로 넣어줘보도록 합시다."],"id":"YIVjwvm428Q_"},{"cell_type":"markdown","metadata":{"id":"FDAaxujL7l1X"},"source":["`input_ids`를 보시면 `cls_token`에 해당하는 2번 토큰이 가장 좌측에 붙게 되며, `sep_token`의 3번 토큰이 각각 중간과 가장 우측에 더해진 것을 확인할 수 있습니다.\n"],"id":"FDAaxujL7l1X"},{"cell_type":"code","execution_count":null,"metadata":{"id":"vD3NpZ-6r7Ks","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684586957553,"user_tz":-540,"elapsed":9,"user":{"displayName":"a jin","userId":"10861403131617240942"}},"outputId":"c3a41370-ed42-4ffd-deb9-d5c29f913aa4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["AlbertTokenizer(name_or_path='bongsoo/albert-small-kor-v1', vocab_size=30000, model_max_length=128, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': AddedToken(\"[MASK]\", rstrip=False, lstrip=True, single_word=False, normalized=False)}, clean_up_tokenization_spaces=True)"]},"metadata":{},"execution_count":13}],"source":["tokenizer"],"id":"vD3NpZ-6r7Ks"},{"cell_type":"markdown","metadata":{"id":"fMDIRs1M4-5a"},"source":["이제 *key* 도 확인이 되었으니, 데이터셋에서 각 예제들을 뽑아와 토큰화 할 수 있는 함수를 아래와 같이 정의해줍니다.\n","\n","해당 함수는 모델을 훈련하기 앞서 데이터셋을 미리 토큰화 시켜놓는 작업을 위한 콜백 함수로 사용되게 됩니다.\n","\n","인자로 넣어주는 `truncation`는 모델이 입력 받을 수 있는 최대 길이 이상의 토큰 시퀀스가 들어오게 될 경우, 최대 길이 기준으로 시퀀스를 자르라는 의미를 지닙니다.\n","\n","( \\* `return_token_type_ids`는 토크나이저가 `token_type_ids`를 반환하도록 할 것인지를 결정하는 인자입니다. `transformers==4.7.0` 기준으로 `token_type_ids`가 기본적으로 반환되므로 `token_type_ids` 자체를 사용하지 않는 `RoBERTa` 모델을 활용하기 위해 해당 인자를 `False`로 설정해주도록 합니다.)"],"id":"fMDIRs1M4-5a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2f8f8cc0"},"outputs":[],"source":["def preprocess_function(examples):\n","    return tokenizer(\n","        examples['message'],\n","        truncation=True,\n","        return_token_type_ids=False,\n","    )"],"id":"2f8f8cc0"},{"cell_type":"markdown","metadata":{"id":"F-qK5fgl4_6t"},"source":["앞서 정의한 `process_function`은 여러 개의 예제 데이터를 받을 수도 있습니다."],"id":"F-qK5fgl4_6t"},{"cell_type":"markdown","metadata":{"id":"a_usMC7U5AyW"},"source":["이제 정의된 전처리 함수를 활용해 데이터셋을 미리 토큰화시키는 작업을 수행합니다.\n","\n","`datasets` 라이브러리를 통해 얻어진 `DatasetDict` 객체는 `map()` 함수를 지원하므로, 정의된 전처리 함수를 데이터셋 토큰화를 위한 콜백 함수로 `map()` 함수 인자로 넘겨주면 됩니다.\n","\n","보다 자세한 내용은 [문서](https://huggingface.co/docs/datasets/processing.html#processing-data-with-map)를 참조해주시면 됩니다."],"id":"a_usMC7U5AyW"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f7c72286","colab":{"base_uri":"https://localhost:8080/","height":17,"referenced_widgets":["d548056bca17453c8f41fb1ab21584e8","761a3fb43a2448eca0afec0fea552121","eca7bd7bcc754048ad3ea7570e6cb955","d2cae06c3e1d471face1fd7554831d86","afc4ae98f29c48828e0a8f5d19fd91d2","078eb48fce19414ca98ed0d404bde047","cca190e574f04950915b08eb659cc3c9","303fea3e2ba0448882a9572ed7dfa5d1","7a975e1dd38a47759375e023be7d7f69","5ac95926297b4668b25c5cc4993c070c","9ed1e0262ac84f35bc36556d17915300","d31402cc78f94225b6f4deabaf5690ec","441ed55f3570409cbb0e9e7da04409a4","a8def265248e4a47afa25ac5630d84d6","9032f1ca9ecf40daa59167eb3a26c22f","61af257422594a3e8ff3753e793fffe2","e8049a49ff6c468f93f3a1d2cec52f83","ce0591bb3aa24468bfd88084a17cb4b7","53a471d546274deebfe7739630e30a44","6d67caec0bb84c0ba06e103e614f8161","bb8c6618301d4b02b0f8a72d4a117968","61ce6a22b97f4691b882611142743e8c"]},"executionInfo":{"status":"ok","timestamp":1684586960650,"user_tz":-540,"elapsed":3105,"user":{"displayName":"a jin","userId":"10861403131617240942"}},"outputId":"22150041-eeca-44bc-fa1d-03d4521c080a"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/7499 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d548056bca17453c8f41fb1ab21584e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d31402cc78f94225b6f4deabaf5690ec"}},"metadata":{}}],"source":["encoded_datasets = datasets.map(preprocess_function, batched=True)"],"id":"f7c72286"},{"cell_type":"code","source":["encoded_datasets"],"metadata":{"id":"WWclUd90dhID","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684586960650,"user_tz":-540,"elapsed":11,"user":{"displayName":"a jin","userId":"10861403131617240942"}},"outputId":"f0905a92-b184-4709-a59c-e5fece534f1f"},"id":"WWclUd90dhID","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['message', 'label', 'input_ids', 'attention_mask'],\n","        num_rows: 7499\n","    })\n","    validation: Dataset({\n","        features: ['message', 'label', 'input_ids', 'attention_mask'],\n","        num_rows: 2500\n","    })\n","})"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"n6p7zIL75Bif"},"source":["학습을 위한 모델을 로드할 차례입니다.\n","\n","앞서 살펴본 바와 같이 **KLUE TC**에는 총 7개의 클래스가 존재하므로, 7개의 클래스를 예측할 수 있는 *SequenceClassification* 구조로 모델을 로드하도록 합니다."],"id":"n6p7zIL75Bif"},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ae93dd8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684586961050,"user_tz":-540,"elapsed":407,"user":{"displayName":"a jin","userId":"10861403131617240942"}},"outputId":"c5b0c863-f75c-4250-e7ba-446b86b100a0"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bongsoo/albert-small-kor-v1 were not used when initializing AlbertForSequenceClassification: ['predictions.dense.bias', 'sop_classifier.classifier.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'sop_classifier.classifier.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias']\n","- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at bongsoo/albert-small-kor-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["num_labels = 2\n","model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"],"id":"7ae93dd8"},{"cell_type":"markdown","metadata":{"id":"ZSc4c6K5oR_R"},"source":["모델을 로드할 때 발생하는 경고 문구는 두 가지 의미를 지닙니다.\n","\n","1. *Masked Language Modeling* 을 위해 존재했던 `lm_head`가 현재는 사용되지 않고 있음을 의미합니다.\n","2. 문장 분류를 위한 `classifier` 레이어를 백본 모델 뒤에 이어 붙였으나 아직 훈련이 되지 않았으므로, 학습을 수행해야 함을 의미합니다."],"id":"ZSc4c6K5oR_R"},{"cell_type":"markdown","metadata":{"id":"x6bvKcsv5CPo"},"source":["마지막으로 앞서 정의한 메트릭을 모델 예측 결과에 적용하기 위한 함수를 정의합니다.\n","\n","입력으로 들어오는 `eval_pred`는 [*EvalPrediction*](https://huggingface.co/transformers/internal/trainer_utils.html#transformers.EvalPrediction) 객체이며, 모델의 클래스 별 예측 값과 정답 값을 지닙니다.\n","\n","클래스 별 예측 중 가장 높은 라벨을 `argmax()`를 통해 뽑아낸 후, 정답 라벨과 비교를 하게 됩니다."],"id":"x6bvKcsv5CPo"},{"cell_type":"code","execution_count":null,"metadata":{"id":"cbfb67d3"},"outputs":[],"source":["def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return metric.compute(predictions=predictions, references=labels, average = 'macro')"],"id":"cbfb67d3"},{"cell_type":"code","source":["!pip install --upgrade accelerate\n","!pip install transformers==4.28.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":876},"id":"DjVAd28UuvMN","executionInfo":{"status":"ok","timestamp":1684586991869,"user_tz":-540,"elapsed":30822,"user":{"displayName":"a jin","userId":"10861403131617240942"}},"outputId":"6514f02a-3e5f-4686-8be8-08487338806f"},"id":"DjVAd28UuvMN","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.19.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==4.28.0\n","  Using cached transformers-4.28.0-py3-none-any.whl (7.0 MB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (3.12.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (0.14.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (0.13.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (3.4)\n","Installing collected packages: transformers\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.29.2\n","    Uninstalling transformers-4.29.2:\n","      Successfully uninstalled transformers-4.29.2\n","Successfully installed transformers-4.28.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["transformers"]}}},"metadata":{}}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"864b6408"},"outputs":[],"source":["metric_name = \"f1\"\n","\n","args = TrainingArguments(\n","    \"test-tc\",\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=8,\n","    weight_decay=0.01,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=metric_name,\n",")"],"id":"864b6408"},{"cell_type":"markdown","metadata":{"id":"PcvI6ZHe3Ty6"},"source":["이제 로드한 모델, 인자 관리 클래스, 데이터셋 등을 *Trainer* 클래스를 초기화에 넘겨주도록 합니다.\n","\n","(TIP: Q: 이미 `encoded_datasets`을 만드는 과정에 토큰화가 이루어졌는데 토크나이저를 굳이 넘겨주는 이유가 무엇인가요?,<br>A: 토큰화는 이루어졌지만 학습 과정 시, 데이터를 배치 단위로 넘겨주는 과정에서 배치에 포함된 가장 긴 시퀀스 기준으로 `truncation`을 수행하고 최대 길이 시퀀스 보다 짧은 시퀀스들은 그 길이만큼 `padding`을 수행해주기 위함입니다.)"],"id":"PcvI6ZHe3Ty6"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c91650f2"},"outputs":[],"source":["trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=encoded_datasets[\"train\"],\n","    eval_dataset=encoded_datasets[\"validation\"],\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")"],"id":"c91650f2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"73261f8e","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1684587591529,"user_tz":-540,"elapsed":593919,"user":{"displayName":"a jin","userId":"10861403131617240942"}},"outputId":"20de65f0-72d7-4ba3-a171-491780ec94d2"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  super().__init__(params, defaults)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='899' max='944' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [899/944 09:48 < 00:29, 1.52 it/s, Epoch 7.61/8]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.332195</td>\n","      <td>0.853990</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.283180</td>\n","      <td>0.882751</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.306087</td>\n","      <td>0.873365</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>0.316461</td>\n","      <td>0.877848</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.245400</td>\n","      <td>0.321669</td>\n","      <td>0.889954</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.245400</td>\n","      <td>0.358225</td>\n","      <td>0.893192</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.245400</td>\n","      <td>0.412260</td>\n","      <td>0.889943</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n","\u001b[31m│\u001b[0m in \u001b[92m<cell line: 1>\u001b[0m:\u001b[94m1\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1664\u001b[0m in \u001b[92mtrain\u001b[0m                    \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1661 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1662 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1663 \u001b[0m\u001b[2m│   │   │   \u001b[0margs=args,                                                                    \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1664 \u001b[2m│   │   │   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,                                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1665 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrial=trial,                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1666 \u001b[0m\u001b[2m│   │   │   \u001b[0mignore_keys_for_eval=ignore_keys_for_eval,                                    \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1667 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1940\u001b[0m in \u001b[92m_inner_training_loop\u001b[0m     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1937 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mtr_loss += tr_loss / (\u001b[94m1\u001b[0m + \u001b[96mself\u001b[0m.state.global_step - \u001b[96mself\u001b[0m._globalstep_  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1938 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1939 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mtr_loss += tr_loss_step                                               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1940 \u001b[2m│   │   │   │   \u001b[0m                                                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1941 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.current_flos += \u001b[96mfloat\u001b[0m(\u001b[96mself\u001b[0m.floating_point_ops(inputs))               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1942 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1943 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# Optimizer step for deepspeed must be called on every step regardless o\u001b[0m  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2753\u001b[0m in \u001b[92mtraining_step\u001b[0m            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2750 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2751 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m (loss, outputs) \u001b[94mif\u001b[0m return_outputs \u001b[94melse\u001b[0m loss                                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2752 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2753 \u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mis_local_process_zero\u001b[0m(\u001b[96mself\u001b[0m) -> \u001b[96mbool\u001b[0m:                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2754 \u001b[0m\u001b[2;90m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2755 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33mWhether or not this process is the local (e.g., on one machine if training in a \u001b[0m  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2756 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33mmachines) main process.\u001b[0m                                                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m487\u001b[0m in \u001b[92mbackward\u001b[0m                         \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 484 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcreate_graph=create_graph,                                                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 485 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs=inputs,                                                            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 486 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 487 \u001b[2m│   │   \u001b[0mtorch.autograd.backward(                                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 488 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=inputs                     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 489 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m 490 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m200\u001b[0m in \u001b[92mbackward\u001b[0m               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# The reason we repeat same the comment below is that\u001b[0m                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m200 \u001b[2m│   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run the bac\u001b[0m   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   │   \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m│   │   \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine to ru\u001b[0m   \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n","\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n","\u001b[1;91mKeyboardInterrupt\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 1&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1664</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1661 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1662 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> inner_training_loop(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1663 │   │   │   </span>args=args,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1664 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>resume_from_checkpoint=resume_from_checkpoint,                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1665 │   │   │   </span>trial=trial,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1666 │   │   │   </span>ignore_keys_for_eval=ignore_keys_for_eval,                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1667 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1940</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_training_loop</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1937 │   │   │   │   │   </span>tr_loss += tr_loss / (<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> + <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.state.global_step - <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._globalstep_  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1938 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1939 │   │   │   │   │   </span>tr_loss += tr_loss_step                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1940 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1941 │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.current_flos += <span style=\"color: #00ffff; text-decoration-color: #00ffff\">float</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.floating_point_ops(inputs))               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1942 │   │   │   │   </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1943 │   │   │   │   # Optimizer step for deepspeed must be called on every step regardless o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2753</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">training_step</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2750 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2751 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> (loss, outputs) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> return_outputs <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> loss                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2752 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2753 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">is_local_process_zero</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>) -&gt; <span style=\"color: #00ffff; text-decoration-color: #00ffff\">bool</span>:                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2754 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2755 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">Whether or not this process is the local (e.g., on one machine if training in a </span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2756 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">machines) main process.</span>                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">487</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 484 │   │   │   │   </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 485 │   │   │   │   </span>inputs=inputs,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 486 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 487 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.autograd.backward(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 488 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=inputs                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 489 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 490 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">200</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">197 │   # The reason we repeat same the comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">198 │   # some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 │   # calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>200 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the bac</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">201 │   │   </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">202 │   │   </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to ru</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">203 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n","</pre>\n"]},"metadata":{}}],"source":["trainer.train()"],"id":"73261f8e"},{"cell_type":"markdown","metadata":{"id":"avS6mBu8779g"},"source":["*Trainer* 는 학습을 마치게 되면, `load_best_model_at_end=True` 인자에 따라 메트릭 기준 가장 좋은 성능을 보였던 체크포인트를 로드하게 됩니다.\n","\n","본 노트북에서는 마지막 에폭 때 가장 좋은 성능을 얻었기에 `evaluate`를 수행해도 같은 결과가 나오겠습니다."],"id":"avS6mBu8779g"},{"cell_type":"code","execution_count":null,"metadata":{"id":"58239e17"},"outputs":[],"source":["# trainer.evaluate()"],"id":"58239e17"},{"cell_type":"code","source":["# import os\n","# os.system(\"mv /content/test-tc/checkpoint-590 /content/drive/MyDrive/텍스트분석2023/model/\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nv7imKutTOr_","executionInfo":{"status":"ok","timestamp":1684588578871,"user_tz":-540,"elapsed":1226,"user":{"displayName":"a jin","userId":"10861403131617240942"}},"outputId":"ee94a307-9669-4650-d87b-ac01927b79c3"},"id":"nv7imKutTOr_","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"YTmKgdOcn6zh"},"source":["## 추론\n"],"id":"YTmKgdOcn6zh"},{"cell_type":"code","source":["# !pip install --upgrade transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":577},"id":"mh1whiBuUJnv","executionInfo":{"status":"ok","timestamp":1684588813362,"user_tz":-540,"elapsed":9412,"user":{"displayName":"a jin","userId":"10861403131617240942"}},"outputId":"fdbc18f5-12d9-446c-ca1b-7f0b7a04596f"},"id":"mh1whiBuUJnv","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.0)\n","Collecting transformers\n","  Using cached transformers-4.29.2-py3-none-any.whl (7.1 MB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: transformers\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.28.0\n","    Uninstalling transformers-4.28.0:\n","      Successfully uninstalled transformers-4.28.0\n","Successfully installed transformers-4.29.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["transformers"]}}},"metadata":{}}]},{"cell_type":"code","source":["# pip install numpy==1.24.2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":353},"id":"cHORVy7xWTMI","executionInfo":{"status":"ok","timestamp":1684589341790,"user_tz":-540,"elapsed":13005,"user":{"displayName":"a jin","userId":"10861403131617240942"}},"outputId":"1e4b47f6-f1e7-4550-f1bd-cfbc088603ea"},"id":"cHORVy7xWTMI","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting numpy==1.24.2\n","  Downloading numpy-1.24.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.22.4\n","    Uninstalling numpy-1.22.4:\n","      Successfully uninstalled numpy-1.22.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.2 which is incompatible.\n","tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.24.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]}}},"metadata":{}}]},{"cell_type":"code","source":["pip install xformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2BnzNM-uXJn-","executionInfo":{"status":"ok","timestamp":1684765194538,"user_tz":-540,"elapsed":168423,"user":{"displayName":"a jin","userId":"10861403131617240942"}},"outputId":"529003ce-6cb2-4887-ea56-5c43a0b26b14"},"id":"2BnzNM-uXJn-","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting xformers\n","  Downloading xformers-0.0.19-cp310-cp310-manylinux2014_x86_64.whl (108.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.2/108.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.22.4)\n","Collecting pyre-extensions==0.0.29 (from xformers)\n","  Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\n","Collecting torch==2.0.0 (from xformers)\n","  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-inspect (from pyre-extensions==0.0.29->xformers)\n","  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyre-extensions==0.0.29->xformers) (4.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->xformers) (3.12.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->xformers) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->xformers) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->xformers) (3.1.2)\n","Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0->xformers)\n","  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0->xformers)\n","  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0->xformers)\n","  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0->xformers)\n","  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0->xformers)\n","  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0->xformers)\n","  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0->xformers)\n","  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0->xformers)\n","  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0->xformers)\n","  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0->xformers)\n","  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0->xformers)\n","  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->xformers) (2.0.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->xformers) (67.7.2)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->xformers) (0.40.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->xformers) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->xformers) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->xformers) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->xformers) (1.3.0)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre-extensions==0.0.29->xformers)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, mypy-extensions, typing-inspect, nvidia-cusolver-cu11, nvidia-cudnn-cu11, pyre-extensions, torch, xformers\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.0.1+cu118\n","    Uninstalling torch-2.0.1+cu118:\n","      Successfully uninstalled torch-2.0.1+cu118\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\n","torchdata 0.6.1 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\n","torchtext 0.15.2 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\n","torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed mypy-extensions-1.0.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pyre-extensions-0.0.29 torch-2.0.0 typing-inspect-0.8.0 xformers-0.0.19\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"y4N2pr-Vnw-M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684765216892,"user_tz":-540,"elapsed":22396,"user":{"displayName":"a jin","userId":"10861403131617240942"}},"outputId":"86220702-916d-45ce-a832-82171bfed347"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n","  warnings.warn(\n"]}],"source":["from transformers import pipeline\n","\n","classifier = pipeline(\n","    \"text-classification\",\n","    model=\"/content/drive/MyDrive/텍스트분석2023/model/checkpoint-590\",\n","    return_all_scores=True,\n",")"],"id":"y4N2pr-Vnw-M"},{"cell_type":"markdown","source":["0 (subjective), 1(objective)"],"metadata":{"id":"IZm5NjgOM5La"},"id":"IZm5NjgOM5La"},{"cell_type":"code","source":["df1['eng'].isnull().unique()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B4yBdBJyVBWU","executionInfo":{"status":"ok","timestamp":1684605785813,"user_tz":-540,"elapsed":7,"user":{"displayName":"a jin","userId":"10861403131617240942"}},"outputId":"ad6bc817-9038-4275-b17b-10b3eac1a32b"},"id":"B4yBdBJyVBWU","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([False])"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["import pandas as pd\n","df1 = pd.read_excel('/content/drive/MyDrive/텍스트분석2023/merge_data/Evinue_translate.xlsx')\n","\n","max_input_length = 512  # 모델의 기대 입력 크기\n","df1_s = []\n","df1_o = []\n","for data in df1['eng']:\n","  if len(data) > max_input_length:\n","    data = data[:max_input_length]  # 입력 데이터를 앞에서부터 max_input_length만큼 잘라냄\n","  tmp = classifier(data)\n","  s = tmp[0][0]['score']\n","  o = tmp[0][1]['score']\n","  df1_s.append(s)\n","  df1_o.append(o)"],"metadata":{"id":"ioDpUCjhSbSD","executionInfo":{"status":"ok","timestamp":1684765447315,"user_tz":-540,"elapsed":228004,"user":{"displayName":"a jin","userId":"10861403131617240942"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c2e1c46f-44ed-46a8-c0f6-ead67253fcb1"},"id":"ioDpUCjhSbSD","execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (181 > 128). Running this sequence through the model will result in indexing errors\n"]}]},{"cell_type":"code","source":["df1['subjective'] = df1_s\n","df1['objective'] = df1_o\n","print(df1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h5AETw2kXzGD","executionInfo":{"status":"ok","timestamp":1684765447320,"user_tz":-540,"elapsed":58,"user":{"displayName":"a jin","userId":"10861403131617240942"}},"outputId":"81dea12d-e5a0-47f3-9554-7228358671f9"},"id":"h5AETw2kXzGD","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                  리뷰 내용  \\\n","0     눈시림 있는 선크림이 많은데 이건 전혀 눈시림이 없어요 바르면 얼굴이 번쩍거릴 정도...   \n","1     한창 쓰다가 다른제품으로 갈아탔다가 이번에 세일하길래 다시 구매했어요 그런데 다른제...   \n","2                       남편 주려고 주문해봤어요바르고 눈 비비니 눈이 따끔거려요   \n","3     헤라 믿쓰화장품 중 하나지요어머니께 공수 받은 애터미 선크림 쓰다가 이것도 가성비갑...   \n","4            냄새는 그닥번들거림 약간있는편톤이 밝아지는건 없음무난한편 눈 따가움 없어좋음   \n","...                                                 ...   \n","1045  와 인생선크림 찾기가 이렇게 힘든가요좋다는거 다써봤지만 왜 헤라꺼는 안샀었는지 후회...   \n","1046                             밀착감이 너무 좋고 자극이 없어서 좋아요   \n","1047       잘발리고 가볍고 좋아요 텁텁하지도 않고 많이 건조하지도 않고 적당하고 딱 좋아요   \n","1048  어떤것들은 기초후에 바르면 밀려서 닦아내곤 하는데 이건 발림성 도좋고 촉촉하니 랑큼...   \n","1049  메이크업 쪽은 잘 몰라서 10년 째 그냥 헤라 제품 사용 합니다톤 업이라 해서 살짝...   \n","\n","                                                    eng  subjective  objective  \n","0     There are a lot of sun creams that hurt my eye...    0.982125   0.017875  \n","1     I used it for a while and changed to another p...    0.988025   0.011975  \n","2     I ordered it for my husbandMy eyes sting when ...    0.980334   0.019666  \n","3     It's one of Hera's MIDZY cosmeticsI was using ...    0.998934   0.001065  \n","4     The smell is not very shiny, but there is no b...    0.995020   0.004980  \n","...                                                 ...         ...        ...  \n","1045  Wow, it's so hard to find the best sun cream. ...    0.994990   0.005010  \n","1046  It sticks to your skin and it doesn't irritate...    0.971932   0.028068  \n","1047  It applies well and it's light and good. It's ...    0.992687   0.007313  \n","1048  Some of them rub off after applying the founda...    0.996994   0.003006  \n","1049  I don't know much about makeup, so I've been u...    0.997734   0.002266  \n","\n","[1050 rows x 4 columns]\n"]}]},{"cell_type":"code","source":["df1.to_excel('/content/drive/MyDrive/텍스트분석2023/merge_data/Evinue_subobj.xlsx')"],"metadata":{"id":"OngrS_JoX-ya","executionInfo":{"status":"ok","timestamp":1684765447962,"user_tz":-540,"elapsed":676,"user":{"displayName":"a jin","userId":"10861403131617240942"}}},"id":"OngrS_JoX-ya","execution_count":7,"outputs":[]},{"cell_type":"code","source":["df2 = pd.read_excel('/content/drive/MyDrive/텍스트분석2023/merge_data/NoEvinue_translate.xlsx')\n","df2_s = []\n","df2_o = []\n","for data in df2['eng']:\n","  if len(data) > max_input_length:\n","    data = data[:max_input_length]  # 입력 데이터를 앞에서부터 max_input_length만큼 잘라냄\n","  tmp = classifier(data)\n","  s = tmp[0][0]['score']\n","  o = tmp[0][1]['score']\n","  df2_s.append(s)\n","  df2_o.append(o)"],"metadata":{"id":"0I3qhi-TTsoO","executionInfo":{"status":"ok","timestamp":1684765903794,"user_tz":-540,"elapsed":159181,"user":{"displayName":"a jin","userId":"10861403131617240942"}}},"id":"0I3qhi-TTsoO","execution_count":12,"outputs":[]},{"cell_type":"code","source":["df2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"qPJ6i01L2Pi8","executionInfo":{"status":"ok","timestamp":1684765479522,"user_tz":-540,"elapsed":10,"user":{"displayName":"a jin","userId":"10861403131617240942"}},"outputId":"6e971059-93f6-4ba4-ac0d-38142b74da2c"},"id":"qPJ6i01L2Pi8","execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      Unnamed: 0                                              리뷰 내용\n","0              0  주변에서 추천을 너무 많이 하셔서 한번 사봤는데 대만족입니다선크림 특히나 톤업 선크...\n","1              1  마스크도 벗고 야외활동도 가능해진 요즘 화장은 필수죠 그래서 달바를 선택했어요 이제...\n","2              2  엄청 더운 요즘 정말이지 가만히 있어도 땀이 줄줄줄아이 학교 왔다갔다 둘째 얼집왔다...\n","3              3  날씨가 풀리면서낮에 아기데리고 산책도 자주하고요즘 햇볕도 많이 뜨거워졌네요 화장도 ...\n","4              4  2개를 번갈아 가면서 사용중인데 워터풀 선크림은 만족스럽습니다 보습력이 좋아요 근데...\n","...          ...                                                ...\n","1045        1045  다이브인세럼을 여러개 쓰고 좋아서 이것도 사봤어요 원래 쓰던것도 좋았는데 토리든 성...\n","1046        1046                    바르면 무거운느낌은 아니고 가볍게 데일리로 바를만 합니다\n","1047        1047  배송속도 하루 지방도시 포장상태 쿠팡 비닐포장사용자 피부타입 민감 수부지 붉은기 색...\n","1048        1048  성인여드름으로 sns에 뜨는 여드름에 좋다고 하면 혹해서 샀었는데 저는 어느 것도 ...\n","1049        1049  쿠팡 체험단 후기입니다 저는 민감성 피부이며 홍조피부에요 아무리 좋다는 제품발라도 ...\n","\n","[1050 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-9f6d44f3-a060-48e7-bec8-9e5146981376\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>리뷰 내용</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>주변에서 추천을 너무 많이 하셔서 한번 사봤는데 대만족입니다선크림 특히나 톤업 선크...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>마스크도 벗고 야외활동도 가능해진 요즘 화장은 필수죠 그래서 달바를 선택했어요 이제...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>엄청 더운 요즘 정말이지 가만히 있어도 땀이 줄줄줄아이 학교 왔다갔다 둘째 얼집왔다...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>날씨가 풀리면서낮에 아기데리고 산책도 자주하고요즘 햇볕도 많이 뜨거워졌네요 화장도 ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>2개를 번갈아 가면서 사용중인데 워터풀 선크림은 만족스럽습니다 보습력이 좋아요 근데...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1045</th>\n","      <td>1045</td>\n","      <td>다이브인세럼을 여러개 쓰고 좋아서 이것도 사봤어요 원래 쓰던것도 좋았는데 토리든 성...</td>\n","    </tr>\n","    <tr>\n","      <th>1046</th>\n","      <td>1046</td>\n","      <td>바르면 무거운느낌은 아니고 가볍게 데일리로 바를만 합니다</td>\n","    </tr>\n","    <tr>\n","      <th>1047</th>\n","      <td>1047</td>\n","      <td>배송속도 하루 지방도시 포장상태 쿠팡 비닐포장사용자 피부타입 민감 수부지 붉은기 색...</td>\n","    </tr>\n","    <tr>\n","      <th>1048</th>\n","      <td>1048</td>\n","      <td>성인여드름으로 sns에 뜨는 여드름에 좋다고 하면 혹해서 샀었는데 저는 어느 것도 ...</td>\n","    </tr>\n","    <tr>\n","      <th>1049</th>\n","      <td>1049</td>\n","      <td>쿠팡 체험단 후기입니다 저는 민감성 피부이며 홍조피부에요 아무리 좋다는 제품발라도 ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1050 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f6d44f3-a060-48e7-bec8-9e5146981376')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9f6d44f3-a060-48e7-bec8-9e5146981376 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9f6d44f3-a060-48e7-bec8-9e5146981376');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["new_df2 = df2.drop(labels='Unnamed: 0',axis=1)"],"metadata":{"id":"WHeCqPZU4EJf","executionInfo":{"status":"ok","timestamp":1684766046541,"user_tz":-540,"elapsed":4,"user":{"displayName":"a jin","userId":"10861403131617240942"}}},"id":"WHeCqPZU4EJf","execution_count":18,"outputs":[]},{"cell_type":"code","source":["new_df2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"yw4CXG2G4cOz","executionInfo":{"status":"ok","timestamp":1684766052528,"user_tz":-540,"elapsed":8,"user":{"displayName":"a jin","userId":"10861403131617240942"}},"outputId":"a0b8f627-6cc0-4b2a-a239-ea02ad44b11b"},"id":"yw4CXG2G4cOz","execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                  리뷰 내용  \\\n","0     주변에서 추천을 너무 많이 하셔서 한번 사봤는데 대만족입니다선크림 특히나 톤업 선크...   \n","1     마스크도 벗고 야외활동도 가능해진 요즘 화장은 필수죠 그래서 달바를 선택했어요 이제...   \n","2     엄청 더운 요즘 정말이지 가만히 있어도 땀이 줄줄줄아이 학교 왔다갔다 둘째 얼집왔다...   \n","3     날씨가 풀리면서낮에 아기데리고 산책도 자주하고요즘 햇볕도 많이 뜨거워졌네요 화장도 ...   \n","4     2개를 번갈아 가면서 사용중인데 워터풀 선크림은 만족스럽습니다 보습력이 좋아요 근데...   \n","...                                                 ...   \n","1045  다이브인세럼을 여러개 쓰고 좋아서 이것도 사봤어요 원래 쓰던것도 좋았는데 토리든 성...   \n","1046                    바르면 무거운느낌은 아니고 가볍게 데일리로 바를만 합니다   \n","1047  배송속도 하루 지방도시 포장상태 쿠팡 비닐포장사용자 피부타입 민감 수부지 붉은기 색...   \n","1048  성인여드름으로 sns에 뜨는 여드름에 좋다고 하면 혹해서 샀었는데 저는 어느 것도 ...   \n","1049  쿠팡 체험단 후기입니다 저는 민감성 피부이며 홍조피부에요 아무리 좋다는 제품발라도 ...   \n","\n","                                                    eng  subjective  objective  \n","0     A lot of people recommended it so I bought it ...    0.977986   0.022014  \n","1     Now that you can take off your mask and do out...    0.430656   0.569344  \n","2     It s really hot these days I went to school wh...    0.638594   0.361406  \n","3     As the weather gets warmer I take babies for w...    0.806109   0.193891  \n","4     I m using two alternately but I m satisfied wi...    0.997510   0.002490  \n","...                                                 ...         ...        ...  \n","1045  I used several dive in serum and I bought this...    0.979820   0.020180  \n","1046  It doesn t feel heavy but it s easy to apply o...    0.992903   0.007097  \n","1047  Delivery speed One day local city Packaging st...    0.411176   0.588824  \n","1048  I bought some pimples when I was good in adult...    0.997592   0.002408  \n","1049  This is a review of Coupang Experience Team I ...    0.870535   0.129465  \n","\n","[1050 rows x 4 columns]"],"text/html":["\n","  <div id=\"df-bfe7e6b7-b277-44d0-8fa5-d9444ba6e6ec\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>리뷰 내용</th>\n","      <th>eng</th>\n","      <th>subjective</th>\n","      <th>objective</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>주변에서 추천을 너무 많이 하셔서 한번 사봤는데 대만족입니다선크림 특히나 톤업 선크...</td>\n","      <td>A lot of people recommended it so I bought it ...</td>\n","      <td>0.977986</td>\n","      <td>0.022014</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>마스크도 벗고 야외활동도 가능해진 요즘 화장은 필수죠 그래서 달바를 선택했어요 이제...</td>\n","      <td>Now that you can take off your mask and do out...</td>\n","      <td>0.430656</td>\n","      <td>0.569344</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>엄청 더운 요즘 정말이지 가만히 있어도 땀이 줄줄줄아이 학교 왔다갔다 둘째 얼집왔다...</td>\n","      <td>It s really hot these days I went to school wh...</td>\n","      <td>0.638594</td>\n","      <td>0.361406</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>날씨가 풀리면서낮에 아기데리고 산책도 자주하고요즘 햇볕도 많이 뜨거워졌네요 화장도 ...</td>\n","      <td>As the weather gets warmer I take babies for w...</td>\n","      <td>0.806109</td>\n","      <td>0.193891</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2개를 번갈아 가면서 사용중인데 워터풀 선크림은 만족스럽습니다 보습력이 좋아요 근데...</td>\n","      <td>I m using two alternately but I m satisfied wi...</td>\n","      <td>0.997510</td>\n","      <td>0.002490</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1045</th>\n","      <td>다이브인세럼을 여러개 쓰고 좋아서 이것도 사봤어요 원래 쓰던것도 좋았는데 토리든 성...</td>\n","      <td>I used several dive in serum and I bought this...</td>\n","      <td>0.979820</td>\n","      <td>0.020180</td>\n","    </tr>\n","    <tr>\n","      <th>1046</th>\n","      <td>바르면 무거운느낌은 아니고 가볍게 데일리로 바를만 합니다</td>\n","      <td>It doesn t feel heavy but it s easy to apply o...</td>\n","      <td>0.992903</td>\n","      <td>0.007097</td>\n","    </tr>\n","    <tr>\n","      <th>1047</th>\n","      <td>배송속도 하루 지방도시 포장상태 쿠팡 비닐포장사용자 피부타입 민감 수부지 붉은기 색...</td>\n","      <td>Delivery speed One day local city Packaging st...</td>\n","      <td>0.411176</td>\n","      <td>0.588824</td>\n","    </tr>\n","    <tr>\n","      <th>1048</th>\n","      <td>성인여드름으로 sns에 뜨는 여드름에 좋다고 하면 혹해서 샀었는데 저는 어느 것도 ...</td>\n","      <td>I bought some pimples when I was good in adult...</td>\n","      <td>0.997592</td>\n","      <td>0.002408</td>\n","    </tr>\n","    <tr>\n","      <th>1049</th>\n","      <td>쿠팡 체험단 후기입니다 저는 민감성 피부이며 홍조피부에요 아무리 좋다는 제품발라도 ...</td>\n","      <td>This is a review of Coupang Experience Team I ...</td>\n","      <td>0.870535</td>\n","      <td>0.129465</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1050 rows × 4 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bfe7e6b7-b277-44d0-8fa5-d9444ba6e6ec')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-bfe7e6b7-b277-44d0-8fa5-d9444ba6e6ec button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-bfe7e6b7-b277-44d0-8fa5-d9444ba6e6ec');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["df2['subjective'] = df2_s\n","df2['objective'] = df2_o\n","print(df2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tlLtUqjbYfAI","executionInfo":{"status":"ok","timestamp":1684765922150,"user_tz":-540,"elapsed":9,"user":{"displayName":"a jin","userId":"10861403131617240942"}},"outputId":"2e78b4c2-577b-4b08-d792-2d3286019b10"},"id":"tlLtUqjbYfAI","execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["      Unnamed: 0                                              리뷰 내용  \\\n","0              0  주변에서 추천을 너무 많이 하셔서 한번 사봤는데 대만족입니다선크림 특히나 톤업 선크...   \n","1              1  마스크도 벗고 야외활동도 가능해진 요즘 화장은 필수죠 그래서 달바를 선택했어요 이제...   \n","2              2  엄청 더운 요즘 정말이지 가만히 있어도 땀이 줄줄줄아이 학교 왔다갔다 둘째 얼집왔다...   \n","3              3  날씨가 풀리면서낮에 아기데리고 산책도 자주하고요즘 햇볕도 많이 뜨거워졌네요 화장도 ...   \n","4              4  2개를 번갈아 가면서 사용중인데 워터풀 선크림은 만족스럽습니다 보습력이 좋아요 근데...   \n","...          ...                                                ...   \n","1045        1045  다이브인세럼을 여러개 쓰고 좋아서 이것도 사봤어요 원래 쓰던것도 좋았는데 토리든 성...   \n","1046        1046                    바르면 무거운느낌은 아니고 가볍게 데일리로 바를만 합니다   \n","1047        1047  배송속도 하루 지방도시 포장상태 쿠팡 비닐포장사용자 피부타입 민감 수부지 붉은기 색...   \n","1048        1048  성인여드름으로 sns에 뜨는 여드름에 좋다고 하면 혹해서 샀었는데 저는 어느 것도 ...   \n","1049        1049  쿠팡 체험단 후기입니다 저는 민감성 피부이며 홍조피부에요 아무리 좋다는 제품발라도 ...   \n","\n","                                                    eng  subjective  objective  \n","0     A lot of people recommended it so I bought it ...    0.977986   0.022014  \n","1     Now that you can take off your mask and do out...    0.430656   0.569344  \n","2     It s really hot these days I went to school wh...    0.638594   0.361406  \n","3     As the weather gets warmer I take babies for w...    0.806109   0.193891  \n","4     I m using two alternately but I m satisfied wi...    0.997510   0.002490  \n","...                                                 ...         ...        ...  \n","1045  I used several dive in serum and I bought this...    0.979820   0.020180  \n","1046  It doesn t feel heavy but it s easy to apply o...    0.992903   0.007097  \n","1047  Delivery speed One day local city Packaging st...    0.411176   0.588824  \n","1048  I bought some pimples when I was good in adult...    0.997592   0.002408  \n","1049  This is a review of Coupang Experience Team I ...    0.870535   0.129465  \n","\n","[1050 rows x 5 columns]\n"]}]},{"cell_type":"code","source":["new_df2.to_excel('/content/drive/MyDrive/텍스트분석2023/merge_data/NoEvinue_subobj.xlsx')"],"metadata":{"id":"J_SGy7UlYcLw","executionInfo":{"status":"ok","timestamp":1684766094924,"user_tz":-540,"elapsed":657,"user":{"displayName":"a jin","userId":"10861403131617240942"}}},"id":"J_SGy7UlYcLw","execution_count":20,"outputs":[]},{"cell_type":"code","source":["# classifier(outStr2)"],"metadata":{"id":"54fF9i8ufHRl","executionInfo":{"status":"ok","timestamp":1684590473788,"user_tz":-540,"elapsed":356,"user":{"displayName":"a jin","userId":"10861403131617240942"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"cb155369-2cd9-493a-fbc3-1837f3fb89dd"},"id":"54fF9i8ufHRl","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[{'label': 'LABEL_0', 'score': 0.9973418116569519},\n","  {'label': 'LABEL_1', 'score': 0.002658255398273468}]]"]},"metadata":{},"execution_count":12}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1Ai1RVuK7pVmSlCGM080OFz832jfxiUUI","timestamp":1684578774833},{"file_id":"12ueyvB_YLd3tfkPv-m6kKKWmI8ojpNR1","timestamp":1681538767032},{"file_id":"11qtug9b787c3hBBjK_ssieM6WD1ikUFv","timestamp":1679188331689},{"file_id":"https://github.com/Huffon/klue-transformers-tutorial/blob/master/natural_language_inference.ipynb","timestamp":1679058086182}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d548056bca17453c8f41fb1ab21584e8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_761a3fb43a2448eca0afec0fea552121","IPY_MODEL_eca7bd7bcc754048ad3ea7570e6cb955","IPY_MODEL_d2cae06c3e1d471face1fd7554831d86"],"layout":"IPY_MODEL_afc4ae98f29c48828e0a8f5d19fd91d2"}},"761a3fb43a2448eca0afec0fea552121":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_078eb48fce19414ca98ed0d404bde047","placeholder":"​","style":"IPY_MODEL_cca190e574f04950915b08eb659cc3c9","value":"Map: 100%"}},"eca7bd7bcc754048ad3ea7570e6cb955":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_303fea3e2ba0448882a9572ed7dfa5d1","max":7499,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7a975e1dd38a47759375e023be7d7f69","value":7499}},"d2cae06c3e1d471face1fd7554831d86":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ac95926297b4668b25c5cc4993c070c","placeholder":"​","style":"IPY_MODEL_9ed1e0262ac84f35bc36556d17915300","value":" 7499/7499 [00:02&lt;00:00, 3283.88 examples/s]"}},"afc4ae98f29c48828e0a8f5d19fd91d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"078eb48fce19414ca98ed0d404bde047":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cca190e574f04950915b08eb659cc3c9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"303fea3e2ba0448882a9572ed7dfa5d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a975e1dd38a47759375e023be7d7f69":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5ac95926297b4668b25c5cc4993c070c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ed1e0262ac84f35bc36556d17915300":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d31402cc78f94225b6f4deabaf5690ec":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_441ed55f3570409cbb0e9e7da04409a4","IPY_MODEL_a8def265248e4a47afa25ac5630d84d6","IPY_MODEL_9032f1ca9ecf40daa59167eb3a26c22f"],"layout":"IPY_MODEL_61af257422594a3e8ff3753e793fffe2"}},"441ed55f3570409cbb0e9e7da04409a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8049a49ff6c468f93f3a1d2cec52f83","placeholder":"​","style":"IPY_MODEL_ce0591bb3aa24468bfd88084a17cb4b7","value":"Map: 100%"}},"a8def265248e4a47afa25ac5630d84d6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_53a471d546274deebfe7739630e30a44","max":2500,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6d67caec0bb84c0ba06e103e614f8161","value":2500}},"9032f1ca9ecf40daa59167eb3a26c22f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb8c6618301d4b02b0f8a72d4a117968","placeholder":"​","style":"IPY_MODEL_61ce6a22b97f4691b882611142743e8c","value":" 2500/2500 [00:00&lt;00:00, 3678.13 examples/s]"}},"61af257422594a3e8ff3753e793fffe2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"e8049a49ff6c468f93f3a1d2cec52f83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce0591bb3aa24468bfd88084a17cb4b7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"53a471d546274deebfe7739630e30a44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d67caec0bb84c0ba06e103e614f8161":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bb8c6618301d4b02b0f8a72d4a117968":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61ce6a22b97f4691b882611142743e8c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"gpuClass":"premium"},"nbformat":4,"nbformat_minor":5}